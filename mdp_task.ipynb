{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 1) Create random MDP task structure given predefined parameters.\n",
    "\n",
    "MDP is defined as 5-tuple $\\left(S,A,P_a,R_a,\\gamma\\right)$. Algorithm creates random transition probability matrices, and reward matrices for each action $a\\in A$. Expected reward depends on realized 3-tuple $(s,a,s')$, where $s,s'\\in S$. Therefore, entire MDP comprised of $N=|S|$ states can be completely represented by four $N\\times N$ square matrices:\n",
    "- `P_left` transition probability matrix after choosing $a=L$\n",
    "- `P_right` transition probability matrix after choosing $a=R$\n",
    "- `Rval_left` and `Runc_left` reward matrix and reward noise matrix after choosing $a=L$\n",
    "- `Rval_right` and `Runc_right` reward matrix and reward noise matrix after choosing $a=R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#=== MDP parameters ===\n",
    "#--- system dynamics\n",
    "N_states = 5 \n",
    "tran_possib = [[.5,.5]]          # possible transition probabilities\n",
    "tran_determ_frac = 0.5           # proportion of deterministic transitions\n",
    "#--- reward dynamics\n",
    "reward_trans_frac = 1/3          # proportion of rewarded transitions \n",
    "reward_noise = [1,2]             # possible reward noise\n",
    "reward_mean = 2                  # mean reward for rewarded transitions\n",
    "\n",
    "#=== TRANSITION PROBABILITY MATRIX ===\n",
    "#--- create empty transition probability matrix for all actions\n",
    "P = np.zeros([2*N_states, N_states])\n",
    "#--- ensure that all states are reachable and escape'able\n",
    "while (0 in np.sum(P, axis=0)) or (2 in np.diag(P[0:N_states]+P[N_states:])):\n",
    "    P = np.zeros([2*N_states, N_states])\n",
    "    #--- number of deterministic transitions for all 2N (s,a) pairs\n",
    "    N_tran_determ = int(np.ceil(2 * N_states * tran_determ_frac))\n",
    "    #--- store transition types (deterministic [code -1] vs. probabilistic [code 0,1,...])\n",
    "    tran_type = np.concatenate(((-1)*np.ones(N_tran_determ),\n",
    "                                [random.randint(0, len(tran_possib)-1) for trans in range(2*N_states - N_tran_determ)]))\n",
    "    np.random.shuffle(tran_type)\n",
    "    #--- fill transition matrices\n",
    "    for s in range(2*N_states):\n",
    "        if tran_type[s] == -1:\n",
    "            #--- deterministic transition to random successor state\n",
    "            P[s][random.randint(0, N_states-1)] = 1\n",
    "        else:\n",
    "            newrow = np.concatenate((tran_possib[int(tran_type[s])],\n",
    "                                    np.zeros(N_states-len(tran_possib[int(tran_type[1])]))))\n",
    "            np.random.shuffle(newrow)\n",
    "            P[s] = newrow\n",
    "#--- divide matrices \n",
    "P_left = P[0:N_states]\n",
    "P_right = P[N_states:]\n",
    "\n",
    "#=== REWARD VALUE and REWARD UNCERTAINTY MATRIX ===\n",
    "N_trans = np.sum(P>0)\n",
    "#--- number of rewarded transitions\n",
    "N_reward_trans = int(np.sum(P>0)*reward_trans_frac)\n",
    "#--- store reward values and their uncertainties\n",
    "reward_val = np.concatenate((np.random.multinomial(N_reward_trans*reward_mean, np.ones(N_reward_trans)/N_reward_trans, size=1)[0],\n",
    "                            np.zeros(N_trans-N_reward_trans)))\n",
    "reward_unc = np.concatenate(([reward_noise[random.randint(0,len(reward_noise)-1)] for i in range(N_reward_trans)],\n",
    "                            np.zeros(N_trans-N_reward_trans)))\n",
    "reward_info = np.vstack([reward_val, reward_unc])\n",
    "reward_info = np.random.permutation(reward_info.T).T\n",
    "#--- create empty reward matrix and reward uncertainty matrix for all actions\n",
    "Rval = np.zeros([2*N_states, N_states])\n",
    "Runc = np.zeros([2*N_states, N_states])\n",
    "#--- loop over transition probability matrix\n",
    "k = 0\n",
    "for idx in range(P.shape[0]):\n",
    "    for idy in range(P.shape[1]):\n",
    "        if P[idx,idy]>0:\n",
    "            Rval[idx,idy] = reward_info[0][k]\n",
    "            Runc[idx,idy] = reward_info[1][k]\n",
    "            k += 1\n",
    "#--- divide matrices\n",
    "Rval_left = Rval[0:N_states]\n",
    "Rval_right = Rval[N_states:]\n",
    "Runc_left = Runc[0:N_states]\n",
    "Runc_right = Runc[N_states:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Loading random stimuli\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "#--- path to stimuli folder\n",
    "stimuli_path = 'stimuli/'\n",
    "\n",
    "#--- get stimuli and randomly draw N_states\n",
    "icon = os.listdir(stimuli_path)\n",
    "shuffle(icon)\n",
    "icon = [f'{stimuli_path}{icon[idx]}' for idx in range(N_states)]\n",
    "\n",
    "def drawstate(icon, index, size):\n",
    "    #--- read image\n",
    "    img = mpimg.imread(icon[index])\n",
    "    #--- draw image\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.imshow(img)\n",
    "    plt.axis('off');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 2) Try out MDP task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load predefined game settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawstate(icon, index, size):\n",
    "    #--- read image\n",
    "    img = mpimg.imread(icon[index])\n",
    "    #--- draw image\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax.imshow(img)\n",
    "    plt.axis('off');\n",
    "    plt.show()\n",
    "    \n",
    "#--- transition probability matrix\n",
    "P_right = [[0   , 0.75, 0.25, 0,    0   ],\n",
    "           [0   , 0   , 0.  , 0.5 , 0.5 ],\n",
    "           [0.25, 0   , 0.75, 0   , 0   ], #self rewarding loop\n",
    "           [0   , 1   , 0.  , 0   , 0   ],\n",
    "           [0   , 0   , 0.  , 1   , 0   ]]\n",
    "P_left = [[1   , 0   , 0   , 0   , 0   ],\n",
    "          [0   , 0.5 , 0   , 0   , 0.5 ],\n",
    "          [1   , 0   , 0   , 0   , 0   ],\n",
    "          [0.  , 0   , 0   , 0   , 1   ],\n",
    "          [0   , 0.5 , 0.5 , 0   , 0   ]]\n",
    "\n",
    "#--- reward matrix\n",
    "Rval_right = [[0 , 0 , 0 , 0 , 0 ],\n",
    "              [0 , 0 , 0 , 1 , 0 ],\n",
    "              [0 , 0 , .5, 0 , 0 ], #self reward magnitude\n",
    "              [0 , 0 , 0 , 0 , 0 ],\n",
    "              [0 , 0 , 0 , 0 , 0 ]]\n",
    "Rval_left = [[0 , 0 , 0 , 0 , 0 ], \n",
    "             [0 , 0 , 0 , 0 , 0 ],\n",
    "             [0 , 0 , 0 , 0 , 0 ],\n",
    "             [0 , 0 , 0 , 0 , -1], #penalty\n",
    "             [0 , 3 , 6 , 0 , 0 ]]\n",
    "\n",
    "#--- reward noise matrix\n",
    "Runc_right = [[0 , 0 , 0 , 0 , 0 ],\n",
    "              [0 , 0 , 0 , 1 , 0 ],\n",
    "              [0 , 0 , 0 , 0 , 0 ], #self rewarding variability \n",
    "              [0 , 0 , 0 , 0 , 0 ],\n",
    "              [0 , 0 , 0 , 0 , 0 ]]\n",
    "Runc_left = [[1 , 0 , 0 , 0 , 0 ], #noisy self-loop\n",
    "             [0 , 0 , 0 , 0 , 0 ],\n",
    "             [0 , 0 , 0 , 0 , 0 ],\n",
    "             [0 , 0 , 0 , 0 , 0 ], #penalty noise \n",
    "             [0 , 1 , 2 , 0 , 0 ]] #big rewards\n",
    "\n",
    "P_right = np.asarray(P_right)\n",
    "P_left = np.asarray(P_left)\n",
    "Rval_right = np.asarray(Rval_right)\n",
    "Rval_left = np.asarray(Rval_left)\n",
    "\n",
    "#--- stimulis \n",
    "icon = ['stimuli/icons8-MouseAnimal-96.png',      # s0\n",
    "        'stimuli/icons8-Chewbacca-96.png',        # s1 \n",
    "        'stimuli/icons8-SockPuppet-96.png',       # s2\n",
    "        'stimuli/icons8-UgandanKnuckles-96.png',  # s3\n",
    "        'stimuli/icons8-Bastet-96.png']           # s4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run code below to play the game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#=== Run task!\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "#--- settings \n",
    "N_trials = 180\n",
    "t_isi = 3\n",
    "\n",
    "#--- reset game history \n",
    "state_history = np.zeros(N_trials+1)\n",
    "reward_history = np.zeros(N_trials)\n",
    "decision_history = []\n",
    "\n",
    "#--- set initial state and reset trial counter\n",
    "trial = 0\n",
    "current_state = random.randint(0,N_states-1)\n",
    "state_history[trial] = current_state\n",
    "\n",
    "for t in range(N_trials):\n",
    "    #=== choice phase ===\n",
    "    clear_output()\n",
    "    print(f'Account: {np.round(sum(reward_history),1)}')\n",
    "    drawstate(icon, current_state, size=3)\n",
    "    decision = 0\n",
    "    while not (decision == '4' or decision == '6'): \n",
    "        decision = input('Choose?')\n",
    "    #====================\n",
    "\n",
    "    #--- choose transition & reward    \n",
    "    if decision == '4': #left\n",
    "        next_state = np.random.choice(N_states, 1, p=P_left[current_state])[0]\n",
    "        r = Rval_left[current_state][next_state] + np.round(np.random.normal(loc=0, scale=Runc_left[current_state][next_state]), 1)\n",
    "    elif decision == '6': #right\n",
    "        next_state = np.random.choice(N_states, 1, p=P_right[current_state])[0]\n",
    "        r = Rval_right[current_state][next_state] + np.round(np.random.normal(loc=0, scale=Runc_right[current_state][next_state]), 1)\n",
    "    #--- update history\n",
    "    reward_history[trial] += r\n",
    "    state_history[trial+1] = next_state\n",
    "    decision_history.append(decision)\n",
    "\n",
    "    #=== outcome phase ===\n",
    "    clear_output()\n",
    "    print(f'Reward: {np.round(r,1)}')\n",
    "    print(f'Account: {np.round(sum(reward_history),1)}')\n",
    "    drawstate(icon, next_state, size=1)\n",
    "    #--- update timestep\n",
    "    trial += 1\n",
    "    current_state = next_state\n",
    "    time.sleep(t_isi)\n",
    "    #====================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Step 3) Solve MDP using value iteration algorithm\n",
    "- Input: system dynamics (`P_right`,`P_left`), reward function (`Rval_left`,`Rval_right`)\n",
    "\n",
    "- Output: optimal policy $\\pi_*$ and value function $v_*$\n",
    "\n",
    "In value iteration following update is performed at each algorithm step \n",
    "$$v_{k+1}(s) = \\max\\limits_{a \\in A}\\left(R^a_s+\\gamma\\sum\\limits_{s'\\in S}P^a_{ss'}v_k\\left( s'\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Show MDP ===\n",
    "system = [P_right, P_left, Rval_right, Rval_left]\n",
    "labels = ['P(a=R)', 'P(a=L)', 'R(a=R)', 'R(a=L)']\n",
    "\n",
    "for idx, mat in enumerate(system):\n",
    "    print(f'\\n=== {labels[idx]} ===')\n",
    "    print(mat)\n",
    "\n",
    "print('\\nState representations.\\n')    \n",
    "    \n",
    "for s in range(N_states):\n",
    "    print(f'State {s}:')\n",
    "    drawstate(icon, s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution found after 32 iterations!\n",
      "Value function = [2.4  4.01 1.68 3.55 6.49]\n",
      "Optimal policy = ['R', 'R', 'L', 'L', 'L']\n"
     ]
    }
   ],
   "source": [
    "gamma = .7 # discount rate \n",
    "value = np.zeros(N_states)\n",
    "\n",
    "#--- implementation details\n",
    "eps = .0001    # stopping condition\n",
    "delta = 2*eps  # initialize \n",
    "iteration = 0  # counter\n",
    "\n",
    "#--- value iteration algorithm\n",
    "while delta > eps:\n",
    "    #--- increment counter\n",
    "    iteration += 1\n",
    "    #--- calculate one step lookahead for both actions\n",
    "    v_left = np.sum(np.multiply(P_left,Rval_left), axis=1) + gamma * np.sum(np.multiply(P_left,value), axis=1)\n",
    "    v_right = np.sum(np.multiply(P_right,Rval_right), axis=1) + gamma * np.sum(np.multiply(P_right,value), axis=1)\n",
    "    #--- max over actions\n",
    "    v_new = np.maximum(v_left,v_right)\n",
    "    #--- update\n",
    "    delta = np.sum(abs(value - v_new))\n",
    "    value = v_new\n",
    "\n",
    "#--- get optimal policy     \n",
    "opt_policy = ['L'*int(b)+'R'*int(not b) for b in v_left > v_right]\n",
    "\n",
    "value = np.round(value, decimals=2)\n",
    "print(f'Solution found after {iteration} iterations!')\n",
    "print(f'Value function = {value}')\n",
    "print(f'Optimal policy = {opt_policy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
